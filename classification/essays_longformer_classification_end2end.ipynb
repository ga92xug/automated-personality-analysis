{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Text classification with the *Longformer*"
      ],
      "metadata": {
        "id": "3mdWDCxWCyBP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYsP5GgWyQwb"
      },
      "source": [
        "\n",
        "\n",
        "In a previous [post](https://jesusleal.io/2020/10/20/RoBERTA-Text-Classification/) I explored how to use Hugging Face Transformers *Trainer* class to easily create a text classification pipeline. The code was pretty straighforward to implement and I was able to obtain results that put the basic model at a very competitive level with a few lines of code. In that post I also briefly discussed one of the main drawbacks of the first generation of Transformers and BERT based architectures; the sequence lenght is limited to a maximum of 512 characters. The reason behind that limitation is the fact that self-attention mechanism scale quadratically with the input sequence length *O(n^2)*. Given the need to process longer sequences of text a a second generation of attention based models have been proposed. The idea behind this models is to reduce the memory footprint of the attention mechanisms in order to process longer sequences of text; see this really useful analysis of transformer models that try to overcome this limitation [by researchers from Google](https://arxiv.org/pdf/2009.06732.pdf). New models such as the [***Reformer***](https://arxiv.org/pdf/2001.04451.pdf) by Google proposes a series of innovations to the traditional Transformer architecture such as Local Self Attention, Locality sensitive hashing (LSH) Self-Attention, Chunked Feed Forward Layers, etc. This [post](https://huggingface.co/blog/reformer) from Hugging Face for a detailed discussion). This model can process sequences of half a million tokens with as little as 8GB of RAM. However one big drawback of the model for downstream applications is the fact that the authors have not released pre trained weights of their model and at the time of publication of this post there is no freely available model pretrained on a large corpus. \n",
        "\n",
        "Another very promising model, and the subject of this post, is the [***Longformer***](https://arxiv.org/pdf/2004.05150.pdf) by researchers from Allen AI Institure. The Longformer allows the processing sequences of thousand characters without facing the memory bottleneck of BERT like architectures and achieved SOTA at the time of publication in several benchmarks. The authors use a new variation of attention, called local attention where every token only attends to tokens in the vicinity defined by a window *w* where each token attends to $\\frac{1}{2}\\ w$  tokens to the left and to the right. To increase the receptive field the authors also applied dilation to the local window so they can increase the size of w without incurring in memory costs. A dilation is simply a \"hole\", meaning the token simply skips that token thus allowing tokens to reach farther tokens. The performance is not hurt since the transformer architecture has multiple attention heads across multiple layers and the different layers and head learn and attend different properties of texts and tokens. In addition to the local attention the authors also included a token that is attended globally so it can be used in downstream taks, just like thee *CLS* token of BERT. One of the interesting aspects of this model is the fact that the authors created their own CUDA kernel to calculate the attention scores of the sliding window attention. This type of attention is more efficient since there are many zeros in the matrix, this operation is called  matrix banded multiplication, but is not implemented in Pytorch/Tensorflow. Thanks to our friends from Hugging Face an implementation with standard CUDA kernels is available altough it does not have all the capabilities the authors of the Longformer model describe in their paper it is suitable for finetuning [downtream tasks](https://github.com/allenai/longformer). \n",
        "\n",
        "\n",
        "The authors tested the model with an autoregressive model to process sequences of thousands of tokens, achieving state of the art on *text8* and *enwik8*. They also tested the model on downstream tasks by finetuning the model with the weights of RoBERTA to conduct masked token prediction (MLM) of one third of the real news dataset, and a third of the stories dataset.  The authors pretrained two variations of the model a base model (with 12 layers) and a large model (30 layers). Both models were trained for 65K gradient updates with sequences of length 4,096 and batch size 64. Once the pretraining was completed they tested the models on downstream tasks such as question answering, coreference resolution and document classification. The model achieved SOTA results on the WikiHop TriviaQA datasets and in the hyper partisan data. For the IMDB dataset the authors achieved 95.7 percent accuracy, a small increase from the 95.3 percent accuracy reported by RoBERTa. \n",
        "\n",
        "Given all this nice features I decided to try the model and see how it compares to RoBERTA on the IMDB the iris dataset of text classification. For this script I used the trainer class from Hugging Face and the pretrained model offered by Allen AI available in the model hub of Hugging Face.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmj22-TcZMef"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kj_7Tz0-pK69",
        "outputId": "a056481e-0e31-4984-839f-df3207ae0e62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.6 MB 7.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 8.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 120 kB 93.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 68.0 MB/s \n",
            "\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tweet-preprocessor\n",
            "  Downloading tweet_preprocessor-0.6.0-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting GPUtil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "Building wheels for collected packages: GPUtil\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7411 sha256=f21a33f87592099a155b033268f9c32b0f178c452b84fb62b87cabf4aeca1da9\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/f8/83/534c52482d6da64622ddbf72cd93c35d2ef2881b78fd08ff0c\n",
            "Successfully built GPUtil\n",
            "Installing collected packages: GPUtil\n",
            "Successfully installed GPUtil-1.4.0\n",
            "Python implementation: CPython\n",
            "Python version       : 3.7.13\n",
            "IPython version      : 7.9.0\n",
            "\n",
            "numpy       : 1.21.6\n",
            "pandas      : 1.3.5\n",
            "torch       : 1.12.1+cu113\n",
            "transformers: 4.21.2\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U watermark\n",
        "!pip install -qq transformers\n",
        "!pip install tqdm\n",
        "!pip install livelossplot --quiet\n",
        "!pip install tweet-preprocessor\n",
        "!pip install GPUtil\n",
        "%reload_ext watermark\n",
        "%watermark -v -p numpy,pandas,torch,transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w68CZpOwFoly"
      },
      "outputs": [],
      "source": [
        "#@title Setup & Config\n",
        "import transformers\n",
        "from transformers import logging\n",
        "logging.set_verbosity_error()\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "from transformers import LongformerConfig, LongformerModel\n",
        "from transformers import RobertaTokenizer\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "\n",
        "\n",
        "import torch\n",
        "from GPUtil import showUtilization as gpu_usage\n",
        "from numba import cuda\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import sklearn\n",
        "import keras\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, GlobalAveragePooling2D, \\\n",
        "Dense, Input, Activation, MaxPool2D\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import csv\n",
        "# import preprocessor as p\n",
        "import math\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "from tensorflow import summary\n",
        "import datetime\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.cuda.amp import autocast \n",
        "%load_ext tensorboard\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "from livelossplot import PlotLosses\n",
        "\n",
        "pd.options.display.max_colwidth = 1000\n",
        "pd.set_option('display.expand_frame_repr', False)\n",
        "\n",
        "import re\n",
        "import imageio,glob\n",
        "\n",
        "import random\n",
        "seed = 0\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "torch.set_printoptions(precision=3, sci_mode=False)\n",
        "\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
        "\n",
        "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
        "\n",
        "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
        "\n",
        "rcParams['figure.figsize'] = 12, 8\n",
        "\n",
        "\n",
        "\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#DEVICE =torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCy6HdlJ7jdx"
      },
      "outputs": [],
      "source": [
        "# declare global settings\n",
        " # this is still possible on the gpu for Bert - 32 not tested yet\n",
        "batch_size = 16\n",
        "PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'\n",
        "#DEVICE =torch.device('cpu')\n",
        "\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#DEVICE =torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_axFRZlUiY0A",
        "outputId": "75c7a3a0-8ca3-4b61-82be-ef528a67628f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Colab Notebooks/application_project/personality-prediction\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "PROJECT_PATH = 'drive/MyDrive/Colab\\ Notebooks/application_project/personality-prediction'\n",
        "%cd $PROJECT_PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Longformer"
      ],
      "metadata": {
        "id": "DiDzM_eu1hSD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkD3w__xyQwg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "#import datasets\n",
        "from transformers import LongformerTokenizerFast, LongformerForSequenceClassification, Trainer, TrainingArguments, LongformerConfig\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from tqdm import tqdm\n",
        "#import wandb\n",
        "import os\n",
        "import preprocessor as p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHhuZ-nxyQwj"
      },
      "source": [
        "One of the cool features about this model is that you can specify the attention sliding window across different levels; the authors exploited this design for the autoregressive language model using different sliding windows for different layers. If this parameter is not changed it will assume a default of 512 across all the different layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKykN7okyQwl",
        "outputId": "c0bff6d6-61bd-49d1-84cc-34ee5167ddc4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LongformerConfig {\n",
              "  \"attention_probs_dropout_prob\": 0.1,\n",
              "  \"attention_window\": 512,\n",
              "  \"bos_token_id\": 0,\n",
              "  \"classifier_dropout\": null,\n",
              "  \"eos_token_id\": 2,\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout_prob\": 0.1,\n",
              "  \"hidden_size\": 768,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 3072,\n",
              "  \"layer_norm_eps\": 1e-12,\n",
              "  \"max_position_embeddings\": 512,\n",
              "  \"model_type\": \"longformer\",\n",
              "  \"num_attention_heads\": 12,\n",
              "  \"num_hidden_layers\": 12,\n",
              "  \"pad_token_id\": 1,\n",
              "  \"position_embedding_type\": \"absolute\",\n",
              "  \"sep_token_id\": 2,\n",
              "  \"transformers_version\": \"4.21.2\",\n",
              "  \"type_vocab_size\": 2,\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 30522\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "config = LongformerConfig()\n",
        "\n",
        "config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3ecw-z2yQwo"
      },
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(sentence):\n",
        "    # remove hyperlinks, hashtags, smileys, emojies\n",
        "    sentence = p.clean(sentence)\n",
        "    # Remove hyperlinks\n",
        "    sentence = re.sub(r\"http\\S+\", \" \", sentence)\n",
        "    # lower case\n",
        "    sentence = sentence.lower()\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z.?!,]', ' ', sentence)\n",
        "    # repeat punctuation\n",
        "    sentence = re.sub(r'(\\.)\\1+', r'\\1', sentence)\n",
        "    sentence = re.sub(r'(!)\\1+', r'\\1', sentence)\n",
        "    sentence = re.sub(r'(\\?)\\1+', r'\\1', sentence) \n",
        "    # Single character removal (except I)\n",
        "    sentence = re.sub(r\"\\s+[a-zA-HJ-Z]\\s+\", ' ', sentence)\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r\"\\s+\", \" \", sentence)\n",
        "    sentence = re.sub(r\"\\|\\|\\|\", \" \", sentence)\n",
        "\n",
        "    return sentence\n",
        "\n",
        "\n",
        "def load_essays_df(datafile):\n",
        "    with open(datafile, \"rt\") as csvf:\n",
        "        csvreader = csv.reader(csvf, delimiter=\",\", quotechar='\"')\n",
        "        first_line = True\n",
        "        df = pd.DataFrame(\n",
        "            columns=[\"user\", \"text\", \"token_len\", \"EXT\", \"NEU\", \"AGR\", \"CON\", \"OPN\"]\n",
        "        )\n",
        "        for line in csvreader:\n",
        "            if first_line:\n",
        "                first_line = False\n",
        "                continue\n",
        "\n",
        "            text = line[1]\n",
        "            df = df.append(\n",
        "                {\n",
        "                    \"user\": line[0],\n",
        "                    \"text\": text,\n",
        "                    \"token_len\": 0,\n",
        "                    \"EXT\": 1 if line[2].lower() == \"y\" else 0,\n",
        "                    \"NEU\": 1 if line[3].lower() == \"y\" else 0,\n",
        "                    \"AGR\": 1 if line[4].lower() == \"y\" else 0,\n",
        "                    \"CON\": 1 if line[5].lower() == \"y\" else 0,\n",
        "                    \"OPN\": 1 if line[6].lower() == \"y\" else 0,\n",
        "                },\n",
        "                ignore_index=True,\n",
        "            )\n",
        "\n",
        "    #print(\"EXT : \", df[\"EXT\"].value_counts())\n",
        "    #print(\"NEU : \", df[\"NEU\"].value_counts())\n",
        "    #print(\"AGR : \", df[\"AGR\"].value_counts())\n",
        "    #print(\"CON : \", df[\"CON\"].value_counts())\n",
        "    #print(\"OPN : \", df[\"OPN\"].value_counts())\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "uNckChhN1z-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_inputs(token_length):\n",
        "    datafile = \"data/essays/essays.csv\"\n",
        "    df = load_essays_df(datafile)\n",
        "\n",
        "    # preprocessing\n",
        "    df['text'] = df['text'].apply(preprocess_text)\n",
        "    # remove empty texts\n",
        "    df = df.drop(df[df.text == ''].index)\n",
        "    df = df.drop(df[df.text == ' '].index)\n",
        "\n",
        "    tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096', max_length = token_length)\n",
        "    # bert encoding\n",
        "    df['encoding'] = df['text'].apply(lambda text: tokenizer.encode_plus(\n",
        "              text,\n",
        "              padding = 'max_length', truncation=True, max_length = token_length\n",
        "              #add_special_tokens=True,\n",
        "              #max_length=token_length,\n",
        "              #pad_to_max_length=True,\n",
        "              #return_token_type_ids=False,\n",
        "              #return_attention_mask=True,\n",
        "          ))\n",
        "\n",
        "    df['input_ids'] = df['encoding'].apply(lambda x: np.asarray(x.input_ids))\n",
        "    df['attention_mask'] = df['encoding'].apply(lambda x: np.asarray(x.attention_mask))\n",
        "    df['author_ids'] = range(1, len(df) + 1)\n",
        "    df['targets'] = df[[\"EXT\", \"NEU\", \"AGR\", \"CON\", \"OPN\"]].values.tolist()\n",
        "\n",
        "    input_ids = df.input_ids.to_numpy()\n",
        "    attention_mask = df.attention_mask.to_numpy()\n",
        "    author_ids = df.author_ids.to_numpy()\n",
        "    targets_arr = df.targets.to_numpy()\n",
        "    targets_arr = [np.asarray(x) for x in targets_arr]\n",
        "\n",
        "    return author_ids, input_ids, attention_mask, targets_arr\n"
      ],
      "metadata": {
        "id": "HNd0f7_14IQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "np.save('data/essays/input_ids.npy', df.input_ids.to_numpy())\n",
        "np.save('data/essays/attention_mask.npy', df.attention_mask.to_numpy())\n",
        "np.save('data/essays/author_ids.npy', df.author_ids.to_numpy())\n",
        "np.save('data/essays/targets.npy', df.targets.to_numpy())\n",
        "\n",
        "\n",
        "input_ids = np.load(open('data/essays/input_ids.npy', 'rb'), allow_pickle=True)\n",
        "attention_mask = np.load(open('data/essays/attention_mask.npy', 'rb'), allow_pickle=True)\n",
        "author_ids = np.load(open('data/essays/author_ids.npy', 'rb'), allow_pickle=True)\n",
        "targets_arr = np.load(open('data/essays/targets.npy', 'rb'), allow_pickle=True)\n",
        "targets_arr = [np.asarray(x) for x in targets_arr]\n",
        "len(input_ids[0])\n",
        "'''"
      ],
      "metadata": {
        "id": "JJQnmg_j5UfV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "612db52f-1ae9-4eec-d293-be9e06ad402c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nnp.save('data/essays/input_ids.npy', df.input_ids.to_numpy())\\nnp.save('data/essays/attention_mask.npy', df.attention_mask.to_numpy())\\nnp.save('data/essays/author_ids.npy', df.author_ids.to_numpy())\\nnp.save('data/essays/targets.npy', df.targets.to_numpy())\\n\\n\\ninput_ids = np.load(open('data/essays/input_ids.npy', 'rb'), allow_pickle=True)\\nattention_mask = np.load(open('data/essays/attention_mask.npy', 'rb'), allow_pickle=True)\\nauthor_ids = np.load(open('data/essays/author_ids.npy', 'rb'), allow_pickle=True)\\ntargets_arr = np.load(open('data/essays/targets.npy', 'rb'), allow_pickle=True)\\ntargets_arr = [np.asarray(x) for x in targets_arr]\\nlen(input_ids[0])\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# one target\n",
        "class Bert_Dataset(Dataset):\n",
        "    def __init__(self, author_ids, input_ids, attention_masks, targets, trait_idx):\n",
        "        input_ids = [np.asarray(x) for x in input_ids]\n",
        "        attention_masks = [np.asarray(x) for x in attention_masks]\n",
        "        self.author_ids = torch.from_numpy(np.array(author_ids))\n",
        "        self.input_ids = torch.from_numpy(np.array(input_ids))\n",
        "        self.attention_masks = torch.from_numpy(np.array(attention_masks))\n",
        "        #targets = targets['EXT']\n",
        "        #one_hot_encoding = tf.keras.utils.to_categorical(targets.to_numpy(), num_classes=2)\n",
        "        #self.targets = torch.from_numpy(one_hot_encoding).float()\n",
        "        #print(targets)\n",
        "        self.targets = torch.from_numpy(targets.to_numpy())[:,trait_idx]\n",
        "\n",
        "        self.global_attention_mask = torch.zeros_like(self.input_ids)\n",
        "        # global attention on cls token\n",
        "        self.global_attention_mask[:, 0] = 1\n",
        "        #print(f'input_ids: {self.input_ids.size()}')\n",
        "        #print(f'attention_mask: {self.attention_masks.size()}')\n",
        "        #print(f'targets: {self.targets.size()}')\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #return (self.author_ids[idx], self.input_ids[idx].to(DEVICE), self.attention_masks[idx].to(DEVICE), self.targets[idx].to(DEVICE))\n",
        "        return {'input_ids': self.input_ids[idx], 'attention_mask': self.attention_masks[idx], 'global_attention_mask': self.global_attention_mask[idx], 'label': self.targets[idx]}"
      ],
      "metadata": {
        "id": "NKUMHTBn7KeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataloader(trait_idx, token_length):\n",
        "\n",
        "    author_ids, input_ids, attention_mask, targets_arr = get_inputs(token_length)\n",
        "\n",
        "    tokenized_df = pd.DataFrame(list(zip(author_ids, input_ids, attention_mask)),\n",
        "                  columns =['author_ids', 'input_ids', 'attention_masks']).apply(np.asarray)\n",
        "    target_df = pd.DataFrame(targets_arr, columns = [\"EXT\", \"NEU\", \"AGR\", \"CON\", \"OPN\"])\n",
        "\n",
        "    df_inputs_train, df_inputs_test, df_targets_train, df_targets_test = train_test_split(tokenized_df, target_df, test_size=0.1, stratify=target_df)\n",
        "\n",
        "    # testing\n",
        "    test_on = 10\n",
        "    inputs = df_inputs_train.iloc[:test_on,]\n",
        "    targets = df_targets_train.iloc[:test_on,]\n",
        "\n",
        "    inputs_train, inputs_val, targets_train, targets_val = train_test_split(inputs, targets, test_size=0.5)\n",
        "    #auth = inputs_train['author_ids']\n",
        "    #tar = targets_train\n",
        "    #print(f'author_ids: \\n{auth}\\ntargets: \\n{tar}')\n",
        "\n",
        "    # dataloader\n",
        "    train_dataset_small = Bert_Dataset(inputs_train['author_ids'].to_numpy(), inputs_train['input_ids'].to_numpy(), inputs_train['attention_masks'].to_numpy(), targets_train, trait_idx)\n",
        "    val_dataset_small = Bert_Dataset(inputs_val['author_ids'].to_numpy(), inputs_val['input_ids'].to_numpy(), inputs_val['attention_masks'].to_numpy(), targets_val, trait_idx)\n",
        "    train_dataloader_small = DataLoader(train_dataset_small, batch_size = batch_size, shuffle = False)\n",
        "    val_dataloader_small = DataLoader(val_dataset_small, batch_size = batch_size)\n",
        "\n",
        "\n",
        "    # normal\n",
        "    inputs = df_inputs_train#.iloc[:test_on,]\n",
        "    targets = df_targets_train#.iloc[:test_on,]\n",
        "\n",
        "    inputs_train, inputs_val, targets_train, targets_val = train_test_split(inputs, targets, test_size=0.15, stratify=targets)\n",
        "\n",
        "    # dataloader\n",
        "    train_dataset = Bert_Dataset(inputs_train['author_ids'].to_numpy(), inputs_train['input_ids'].to_numpy(), inputs_train['attention_masks'].to_numpy(), targets_train, trait_idx)\n",
        "    val_dataset = Bert_Dataset(inputs_val['author_ids'].to_numpy(), inputs_val['input_ids'].to_numpy(), inputs_val['attention_masks'].to_numpy(), targets_val, trait_idx)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size = batch_size)\n",
        "\n",
        "    return train_dataset, val_dataset, train_dataset_small, val_dataset_small\n"
      ],
      "metadata": {
        "id": "sDtatvEU7q58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dic = iter(train_dataloader_small).next()\n",
        "#dic['label'].dtype == torch.long"
      ],
      "metadata": {
        "id": "WpxtDAfWBxtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d7iY7QRuyQwp",
        "outputId": "be7933d9-630a-4ef6-9f76-249ae33aaf3a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ntrain_data, test_data = datasets.load_dataset('imdb', split =['train', 'test'], \\n                                             cache_dir='/media/data_files/github/website_tutorials/data')\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "'''\n",
        "train_data, test_data = datasets.load_dataset('imdb', split =['train', 'test'], \n",
        "                                             cache_dir='/media/data_files/github/website_tutorials/data')\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "id": "RVESHAvuyQws",
        "outputId": "c35ce884-4b5c-4a96-81a6-b549208c0d46"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntrain_data\\n#dir(train_data)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "'''\n",
        "train_data\n",
        "#dir(train_data)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvKfKmQDyQwu"
      },
      "source": [
        "For my implementation of the model, and to save speed in the pretraining I chose the maximun length of 1024 characters which covers close to 98 percent of all the documents in the dataset. Before using my brand new and still pretty much impossible to find RTX3090, I set the gradient checkpointing parameter to true. This saves a huge amount of memory and allows models such as the longformer to train on more modest GPU's such as my old EVGA GeForce GTX 1080. Gradient checkpointing is a really nice way to re use weights in the neural network and allows massive models to run on more modest settings with a 30 percent increase in training time. The original paper discussing gradient checkpointing can be found [here](https://arxiv.org/pdf/1604.06174.pdf) and a nice [discussion of gradient checkpointing can be hound here](https://qywu.github.io/2019/05/22/explore-gradient-checkpointing.html). \n",
        "\n",
        "Additionally to save memory and increase training time I also used mixed precision training to speed up the computation time of the training process. If you want to learn more about mixed precision I recommend this [blogpost](https://jonathan-hui.medium.com/mixed-precision-in-deep-learning-67f6dce3e0f3). With the combination of mixed precision, gradient accumulation and gradient checkpoint you can set the length to 4096. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehcJ0rv1yQwv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213,
          "referenced_widgets": [
            "f2ca6bc2131f4458b6bccdd7bb46e57a",
            "47534b1b9b7a4fb5bc747b1934c809ac",
            "82b84e4c1a8e4371a0607b310478de7e",
            "b4275a92abdf413188c0399155dd302a",
            "6db49a14f61247e5b284de66782ef265",
            "b659999f3bdb492983086c9f435dd9f8",
            "95b1d6b2808846678262befbacbaad5a",
            "49eccabc9bd24a0eafebaf50c0da1c3f",
            "2589d4786db94ad8b5f2ef8cfc086eda",
            "e798a493f1ea49158b13ddf01b27d4a6",
            "4bbe99b3010547aab7708f7e51aca631",
            "909c7bc870254134a0fc0db2718a28ca",
            "02f73c01b0914bafbb916e5d966db206",
            "7d5210b45fda46ffb589e0d509a2c7b8",
            "d0e097ee1bdf492e8cc054867734cb29",
            "d640f76596344de484cb8b73c0a3fcbb",
            "e02701a838d64b3b98fa2c05e7a331a8",
            "ba62a78b888c4e71b67eb7f33038fdb9",
            "ed6e24b789b64918a4188d5720b28d36",
            "7c092a119fe743f381b1e5ade9df670f",
            "76abb59735a34bafbff2ccc150637e92",
            "90075bd6cb2b41898fe9f295cffe091b"
          ]
        },
        "outputId": "5fe5c48d-5d30-4e71-a881-61953f9a6de6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f2ca6bc2131f4458b6bccdd7bb46e57a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/570M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "909c7bc870254134a0fc0db2718a28ca"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# load model and tokenizer and define length of the text sequence\n",
        "model = LongformerForSequenceClassification.from_pretrained('allenai/longformer-base-4096',\n",
        "                                                           gradient_checkpointing=False,\n",
        "                                                           attention_window = 512)\n",
        "#tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096', max_length = 1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0hyob-vyQwy",
        "outputId": "7bef4a1f-7ada-4116-f576-756ac786fa1a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LongformerConfig {\n",
              "  \"_name_or_path\": \"allenai/longformer-base-4096\",\n",
              "  \"attention_mode\": \"longformer\",\n",
              "  \"attention_probs_dropout_prob\": 0.1,\n",
              "  \"attention_window\": [\n",
              "    512,\n",
              "    512,\n",
              "    512,\n",
              "    512,\n",
              "    512,\n",
              "    512,\n",
              "    512,\n",
              "    512,\n",
              "    512,\n",
              "    512,\n",
              "    512,\n",
              "    512\n",
              "  ],\n",
              "  \"bos_token_id\": 0,\n",
              "  \"classifier_dropout\": null,\n",
              "  \"eos_token_id\": 2,\n",
              "  \"gradient_checkpointing\": false,\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout_prob\": 0.1,\n",
              "  \"hidden_size\": 768,\n",
              "  \"ignore_attention_mask\": false,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 3072,\n",
              "  \"layer_norm_eps\": 1e-05,\n",
              "  \"max_position_embeddings\": 4098,\n",
              "  \"model_type\": \"longformer\",\n",
              "  \"num_attention_heads\": 12,\n",
              "  \"num_hidden_layers\": 12,\n",
              "  \"pad_token_id\": 1,\n",
              "  \"position_embedding_type\": \"absolute\",\n",
              "  \"sep_token_id\": 2,\n",
              "  \"transformers_version\": \"4.21.2\",\n",
              "  \"type_vocab_size\": 1,\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 50265\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "model.config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jvBkp1WcyQw0",
        "outputId": "5baffbc1-d38c-47c2-e605-eca781d201f6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# define a function that will tokenize the model, and will return the relevant inputs for the model\\ndef tokenization(batched_text):\\n    return tokenizer(batched_text['text'], padding = 'max_length', truncation=True, max_length = 1024)\\n\\ntrain_data = train_data.map(tokenization, batched = True, batch_size = len(train_data))\\ntest_data = test_data.map(tokenization, batched = True, batch_size = len(test_data))\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "'''\n",
        "# define a function that will tokenize the model, and will return the relevant inputs for the model\n",
        "def tokenization(batched_text):\n",
        "    return tokenizer(batched_text['text'], padding = 'max_length', truncation=True, max_length = 1024)\n",
        "\n",
        "train_data = train_data.map(tokenization, batched = True, batch_size = len(train_data))\n",
        "test_data = test_data.map(tokenization, batched = True, batch_size = len(test_data))\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-LB1cMNYyQw1",
        "outputId": "b66cd969-d093-4888-a355-77a9508cd022"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# we make sure our truncation strateging and the padding are set to the maximung length\\nlen(train_data['input_ids'][0])\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "'''\n",
        "# we make sure our truncation strateging and the padding are set to the maximung length\n",
        "len(train_data['input_ids'][0])\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5l9TU3iyQw3"
      },
      "source": [
        "Once the tokenization process is finished we can use the set the column names and types. One thing that is important to note is that the `LongformerForSequenceClassification` implementation by default sets the global attention to the `CLS`[token](https://huggingface.co/transformers/_modules/transformers/modeling_longformer.html#LongformerForSequenceClassification), so there is no need to further modify the inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ic5IcGLHyQw4",
        "outputId": "58b8d218-3bfc-479f-8e3b-1c6502aa78d0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ntrain_data.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\\ntest_data.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "'''\n",
        "train_data.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "test_data.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "RDZV-y5KFEHL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_ulCBOjyQw5"
      },
      "outputs": [],
      "source": [
        "# define accuracy metrics\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    #print(f'labels: {labels}')\n",
        "    #print(f'preds: {preds}')\n",
        "    # argmax(pred.predictions, axis=1)\n",
        "    #pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    #print(f'acc: {acc}')\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Npl8OlYsyQw6"
      },
      "source": [
        "In the paper the authors trained for 15 epochs, with batch size of 32, learning rate of 3e-5 and linear warmup steps equal to 0.1 of the total training steps. For this quick tutorial I went for the default learning rate of the trainer class which is 5e-5, 5 epochs for training, batch size of 8 with gradient accumulation of 8 steps for an effective batch size of 64 and 200 warm up steps (roughly 10 percent of total training steps). The overall training time for this implementation was 2 hours and 54 minutes."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def free_gpu_cache():\n",
        "    print(\"Initial GPU Usage\")\n",
        "    gpu_usage()                             \n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    cuda.select_device(0)\n",
        "    cuda.close()\n",
        "    cuda.select_device(0)\n",
        "\n",
        "    print(\"GPU Usage after emptying the cache\")\n",
        "    gpu_usage()\n",
        "\n",
        "#free_gpu_cache()                           \n"
      ],
      "metadata": {
        "id": "HhN3DJHYGv_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "86bcea883ff7458b987faabc3973ba9f",
            "5af02b66ec6d4b7e8b33af76cecf56c1",
            "d9867a27a719436aa97d96fc09270b97"
          ]
        },
        "id": "ig_yTUnByQw8",
        "outputId": "aa4e1e1c-7386-47b4-a4f3-eb6a6edc6a9f"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trait: EXT\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "86bcea883ff7458b987faabc3973ba9f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading vocab.json:   0%|          | 0.00/878k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5af02b66ec6d4b7e8b33af76cecf56c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d9867a27a719436aa97d96fc09270b97",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading tokenizer.json:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cuda_amp half precision backend\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 1887\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 2\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 16\n",
            "  Total optimization steps = 295\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='295' max='295' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [295/295 2:11:12, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.703100</td>\n",
              "      <td>0.691502</td>\n",
              "      <td>0.522523</td>\n",
              "      <td>0.683897</td>\n",
              "      <td>0.519637</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.706800</td>\n",
              "      <td>0.689798</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.581921</td>\n",
              "      <td>0.565934</td>\n",
              "      <td>0.598837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.688000</td>\n",
              "      <td>0.686444</td>\n",
              "      <td>0.579580</td>\n",
              "      <td>0.539474</td>\n",
              "      <td>0.621212</td>\n",
              "      <td>0.476744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.747000</td>\n",
              "      <td>0.707510</td>\n",
              "      <td>0.579580</td>\n",
              "      <td>0.651741</td>\n",
              "      <td>0.569565</td>\n",
              "      <td>0.761628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.346500</td>\n",
              "      <td>0.842962</td>\n",
              "      <td>0.573574</td>\n",
              "      <td>0.598870</td>\n",
              "      <td>0.582418</td>\n",
              "      <td>0.616279</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 333\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 333\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 333\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 333\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 333\n",
            "  Batch size = 2\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trait: NEU\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/859f4633944e1b7e7fa301e72161388cd5903e36385d0ef2917256506bff64c3.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/af6fcabe2bf8cab6f77b20d94ba46a3dbf441ca0549e1f3c852c437b612f5224.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/93ab433997eab2709f7adf8fa46f21d4699497bf20768f3ffd25e2e73b9b93c2.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/allenai/longformer-base-4096/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/0690955d8f70934f95adf0fb108d5f7322d02f8d7dd938b7b133cb7421e120e6.b25f41ff6acdcb7ab47c505c70e351b3fc01957b3798197e5ac6e8efc547ac99\n",
            "Model config LongformerConfig {\n",
            "  \"_name_or_path\": \"allenai/longformer-base-4096\",\n",
            "  \"attention_mode\": \"longformer\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attention_window\": [\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"ignore_attention_mask\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 4098,\n",
            "  \"model_type\": \"longformer\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sep_token_id\": 2,\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Using cuda_amp half precision backend\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 1887\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 2\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 16\n",
            "  Total optimization steps = 295\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='295' max='295' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [295/295 2:11:13, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.676900</td>\n",
              "      <td>0.716781</td>\n",
              "      <td>0.486486</td>\n",
              "      <td>0.219178</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>0.143713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.648800</td>\n",
              "      <td>0.705923</td>\n",
              "      <td>0.495495</td>\n",
              "      <td>0.508772</td>\n",
              "      <td>0.497143</td>\n",
              "      <td>0.520958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.559700</td>\n",
              "      <td>0.754877</td>\n",
              "      <td>0.471471</td>\n",
              "      <td>0.511111</td>\n",
              "      <td>0.476684</td>\n",
              "      <td>0.550898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.589900</td>\n",
              "      <td>0.921062</td>\n",
              "      <td>0.486486</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.491379</td>\n",
              "      <td>0.682635</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.194400</td>\n",
              "      <td>1.188633</td>\n",
              "      <td>0.486486</td>\n",
              "      <td>0.457143</td>\n",
              "      <td>0.486486</td>\n",
              "      <td>0.431138</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 333\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 333\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 333\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 333\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 333\n",
            "  Batch size = 2\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trait: AGR\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/859f4633944e1b7e7fa301e72161388cd5903e36385d0ef2917256506bff64c3.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/af6fcabe2bf8cab6f77b20d94ba46a3dbf441ca0549e1f3c852c437b612f5224.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/93ab433997eab2709f7adf8fa46f21d4699497bf20768f3ffd25e2e73b9b93c2.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/allenai/longformer-base-4096/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/0690955d8f70934f95adf0fb108d5f7322d02f8d7dd938b7b133cb7421e120e6.b25f41ff6acdcb7ab47c505c70e351b3fc01957b3798197e5ac6e8efc547ac99\n",
            "Model config LongformerConfig {\n",
            "  \"_name_or_path\": \"allenai/longformer-base-4096\",\n",
            "  \"attention_mode\": \"longformer\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attention_window\": [\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"ignore_attention_mask\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 4098,\n",
            "  \"model_type\": \"longformer\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sep_token_id\": 2,\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Using cuda_amp half precision backend\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 1887\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 2\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 16\n",
            "  Total optimization steps = 295\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='295' max='295' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [295/295 2:11:15, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.691000</td>\n",
              "      <td>0.689569</td>\n",
              "      <td>0.549550</td>\n",
              "      <td>0.669604</td>\n",
              "      <td>0.548736</td>\n",
              "      <td>0.858757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.672100</td>\n",
              "      <td>0.684312</td>\n",
              "      <td>0.609610</td>\n",
              "      <td>0.700461</td>\n",
              "      <td>0.591440</td>\n",
              "      <td>0.858757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.723700</td>\n",
              "      <td>0.694002</td>\n",
              "      <td>0.573574</td>\n",
              "      <td>0.579882</td>\n",
              "      <td>0.608696</td>\n",
              "      <td>0.553672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.458300</td>\n",
              "      <td>0.781133</td>\n",
              "      <td>0.516517</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.547619</td>\n",
              "      <td>0.519774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.371900</td>\n",
              "      <td>0.912569</td>\n",
              "      <td>0.525526</td>\n",
              "      <td>0.579787</td>\n",
              "      <td>0.547739</td>\n",
              "      <td>0.615819</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 333\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 333\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 333\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 333\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 333\n",
            "  Batch size = 2\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trait: CON\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/859f4633944e1b7e7fa301e72161388cd5903e36385d0ef2917256506bff64c3.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/af6fcabe2bf8cab6f77b20d94ba46a3dbf441ca0549e1f3c852c437b612f5224.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/93ab433997eab2709f7adf8fa46f21d4699497bf20768f3ffd25e2e73b9b93c2.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/allenai/longformer-base-4096/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/0690955d8f70934f95adf0fb108d5f7322d02f8d7dd938b7b133cb7421e120e6.b25f41ff6acdcb7ab47c505c70e351b3fc01957b3798197e5ac6e8efc547ac99\n",
            "Model config LongformerConfig {\n",
            "  \"_name_or_path\": \"allenai/longformer-base-4096\",\n",
            "  \"attention_mode\": \"longformer\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attention_window\": [\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"ignore_attention_mask\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 4098,\n",
            "  \"model_type\": \"longformer\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sep_token_id\": 2,\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Using cuda_amp half precision backend\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 1887\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 2\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 16\n",
            "  Total optimization steps = 295\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='121' max='295' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [121/295 53:06 < 1:17:39, 0.04 it/s, Epoch 2.03/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.682000</td>\n",
              "      <td>0.695260</td>\n",
              "      <td>0.546547</td>\n",
              "      <td>0.494983</td>\n",
              "      <td>0.569231</td>\n",
              "      <td>0.437870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.640100</td>\n",
              "      <td>0.713680</td>\n",
              "      <td>0.558559</td>\n",
              "      <td>0.672606</td>\n",
              "      <td>0.539286</td>\n",
              "      <td>0.893491</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 333\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 333\n",
            "  Batch size = 2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='295' max='295' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [295/295 2:11:14, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.682000</td>\n",
              "      <td>0.695260</td>\n",
              "      <td>0.546547</td>\n",
              "      <td>0.494983</td>\n",
              "      <td>0.569231</td>\n",
              "      <td>0.437870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.640100</td>\n",
              "      <td>0.713680</td>\n",
              "      <td>0.558559</td>\n",
              "      <td>0.672606</td>\n",
              "      <td>0.539286</td>\n",
              "      <td>0.893491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.662300</td>\n",
              "      <td>0.781187</td>\n",
              "      <td>0.522523</td>\n",
              "      <td>0.589147</td>\n",
              "      <td>0.522936</td>\n",
              "      <td>0.674556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.598700</td>\n",
              "      <td>0.851811</td>\n",
              "      <td>0.534535</td>\n",
              "      <td>0.619165</td>\n",
              "      <td>0.529412</td>\n",
              "      <td>0.745562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.302300</td>\n",
              "      <td>1.203149</td>\n",
              "      <td>0.492492</td>\n",
              "      <td>0.470219</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.443787</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 333\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 333\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 333\n",
            "  Batch size = 2\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trait: OPN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/859f4633944e1b7e7fa301e72161388cd5903e36385d0ef2917256506bff64c3.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/af6fcabe2bf8cab6f77b20d94ba46a3dbf441ca0549e1f3c852c437b612f5224.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/93ab433997eab2709f7adf8fa46f21d4699497bf20768f3ffd25e2e73b9b93c2.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/allenai/longformer-base-4096/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/0690955d8f70934f95adf0fb108d5f7322d02f8d7dd938b7b133cb7421e120e6.b25f41ff6acdcb7ab47c505c70e351b3fc01957b3798197e5ac6e8efc547ac99\n",
            "Model config LongformerConfig {\n",
            "  \"_name_or_path\": \"allenai/longformer-base-4096\",\n",
            "  \"attention_mode\": \"longformer\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attention_window\": [\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"ignore_attention_mask\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 4098,\n",
            "  \"model_type\": \"longformer\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sep_token_id\": 2,\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Using cuda_amp half precision backend\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 1887\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 2\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 16\n",
            "  Total optimization steps = 295\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='295' max='295' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [295/295 2:11:21, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.657700</td>\n",
              "      <td>0.713682</td>\n",
              "      <td>0.504505</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.510836</td>\n",
              "      <td>0.959302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.690500</td>\n",
              "      <td>0.694453</td>\n",
              "      <td>0.540541</td>\n",
              "      <td>0.529231</td>\n",
              "      <td>0.562092</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.678100</td>\n",
              "      <td>0.703114</td>\n",
              "      <td>0.576577</td>\n",
              "      <td>0.593660</td>\n",
              "      <td>0.588571</td>\n",
              "      <td>0.598837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.485300</td>\n",
              "      <td>0.792425</td>\n",
              "      <td>0.567568</td>\n",
              "      <td>0.595506</td>\n",
              "      <td>0.576087</td>\n",
              "      <td>0.616279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.266600</td>\n",
              "      <td>0.977825</td>\n",
              "      <td>0.540541</td>\n",
              "      <td>0.553936</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.552326</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 333\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 333\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 333\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 333\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 333\n",
            "  Batch size = 2\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "trait_labels = [\"EXT\", \"NEU\", \"AGR\", \"CON\", \"OPN\"]\n",
        "for trait_idx, trait in enumerate(trait_labels):\n",
        "    #if trait_idx == 4:\n",
        "     #   continue\n",
        "    print(f'Trait: {trait}')\n",
        "    train_dataset, val_dataset, train_dataset_small, val_dataset_small = get_dataloader(trait_idx = trait_idx, token_length = 4096)\n",
        "\n",
        "    # define the training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir = '/results_longformer',\n",
        "        num_train_epochs = 5,\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 16,    \n",
        "        per_device_eval_batch_size= 2,\n",
        "        evaluation_strategy = \"epoch\",\n",
        "        disable_tqdm = False, \n",
        "        #load_best_model_at_end=True,\n",
        "        warmup_steps=200,\n",
        "        weight_decay=0.01,\n",
        "        logging_steps = 1,\n",
        "        fp16 = True,\n",
        "        logging_dir='/logs_longformer',\n",
        "        dataloader_num_workers = 0,\n",
        "        run_name = 'longformer-classification_2_warm'\n",
        "    )\n",
        "\n",
        "    # instantiate the trainer class and check for available devices\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        compute_metrics=compute_metrics,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset\n",
        "    )\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "id": "PgMUlUlsyQw9",
        "outputId": "23336e5f-b5f0-45b6-93f7-58cdc7c3c5a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 1887\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 8\n",
            "  Total optimization steps = 295\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='295' max='295' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [295/295 37:59, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.685500</td>\n",
              "      <td>0.687596</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.560606</td>\n",
              "      <td>0.645349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.581500</td>\n",
              "      <td>0.696063</td>\n",
              "      <td>0.528529</td>\n",
              "      <td>0.269767</td>\n",
              "      <td>0.674419</td>\n",
              "      <td>0.168605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.617500</td>\n",
              "      <td>0.659777</td>\n",
              "      <td>0.612613</td>\n",
              "      <td>0.562712</td>\n",
              "      <td>0.674797</td>\n",
              "      <td>0.482558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.602700</td>\n",
              "      <td>0.764710</td>\n",
              "      <td>0.576577</td>\n",
              "      <td>0.429150</td>\n",
              "      <td>0.706667</td>\n",
              "      <td>0.308140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.336400</td>\n",
              "      <td>0.857054</td>\n",
              "      <td>0.576577</td>\n",
              "      <td>0.534653</td>\n",
              "      <td>0.618321</td>\n",
              "      <td>0.470930</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 333\n",
            "  Batch size = 4\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 333\n",
            "  Batch size = 4\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 333\n",
            "  Batch size = 4\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 333\n",
            "  Batch size = 4\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 333\n",
            "  Batch size = 4\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=295, training_loss=0.5946918003639933, metrics={'train_runtime': 2289.3531, 'train_samples_per_second': 4.121, 'train_steps_per_second': 0.129, 'total_flos': 6197394955898880.0, 'train_loss': 0.5946918003639933, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "# train the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiw6kMciyQw-"
      },
      "source": [
        "After the training has been completed we can evaluate the performance of the model and make sure we are loading the right model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_if0jdiyQw_",
        "outputId": "362f0f5c-1db3-45ab-b65c-a1168d9924e8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='1563' max='1563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1563/1563 07:15]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.13697753846645355,\n",
              " 'eval_accuracy': 0.9534,\n",
              " 'eval_f1': 0.9535282619968887,\n",
              " 'eval_precision': 0.9509109714376641,\n",
              " 'eval_recall': 0.95616,\n",
              " 'epoch': 4.9984}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.evaluate()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "3mdWDCxWCyBP",
        "wmj22-TcZMef",
        "DiDzM_eu1hSD"
      ]
    },
    "gpuClass": "standard",
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f2ca6bc2131f4458b6bccdd7bb46e57a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_47534b1b9b7a4fb5bc747b1934c809ac",
              "IPY_MODEL_82b84e4c1a8e4371a0607b310478de7e",
              "IPY_MODEL_b4275a92abdf413188c0399155dd302a"
            ],
            "layout": "IPY_MODEL_6db49a14f61247e5b284de66782ef265"
          }
        },
        "47534b1b9b7a4fb5bc747b1934c809ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b659999f3bdb492983086c9f435dd9f8",
            "placeholder": "​",
            "style": "IPY_MODEL_95b1d6b2808846678262befbacbaad5a",
            "value": "Downloading config.json: 100%"
          }
        },
        "82b84e4c1a8e4371a0607b310478de7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49eccabc9bd24a0eafebaf50c0da1c3f",
            "max": 694,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2589d4786db94ad8b5f2ef8cfc086eda",
            "value": 694
          }
        },
        "b4275a92abdf413188c0399155dd302a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e798a493f1ea49158b13ddf01b27d4a6",
            "placeholder": "​",
            "style": "IPY_MODEL_4bbe99b3010547aab7708f7e51aca631",
            "value": " 694/694 [00:00&lt;00:00, 21.9kB/s]"
          }
        },
        "6db49a14f61247e5b284de66782ef265": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b659999f3bdb492983086c9f435dd9f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95b1d6b2808846678262befbacbaad5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "49eccabc9bd24a0eafebaf50c0da1c3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2589d4786db94ad8b5f2ef8cfc086eda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e798a493f1ea49158b13ddf01b27d4a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bbe99b3010547aab7708f7e51aca631": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "909c7bc870254134a0fc0db2718a28ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_02f73c01b0914bafbb916e5d966db206",
              "IPY_MODEL_7d5210b45fda46ffb589e0d509a2c7b8",
              "IPY_MODEL_d0e097ee1bdf492e8cc054867734cb29"
            ],
            "layout": "IPY_MODEL_d640f76596344de484cb8b73c0a3fcbb"
          }
        },
        "02f73c01b0914bafbb916e5d966db206": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e02701a838d64b3b98fa2c05e7a331a8",
            "placeholder": "​",
            "style": "IPY_MODEL_ba62a78b888c4e71b67eb7f33038fdb9",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "7d5210b45fda46ffb589e0d509a2c7b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed6e24b789b64918a4188d5720b28d36",
            "max": 597257159,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7c092a119fe743f381b1e5ade9df670f",
            "value": 597257159
          }
        },
        "d0e097ee1bdf492e8cc054867734cb29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76abb59735a34bafbff2ccc150637e92",
            "placeholder": "​",
            "style": "IPY_MODEL_90075bd6cb2b41898fe9f295cffe091b",
            "value": " 570M/570M [00:08&lt;00:00, 68.2MB/s]"
          }
        },
        "d640f76596344de484cb8b73c0a3fcbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e02701a838d64b3b98fa2c05e7a331a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba62a78b888c4e71b67eb7f33038fdb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed6e24b789b64918a4188d5720b28d36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c092a119fe743f381b1e5ade9df670f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "76abb59735a34bafbff2ccc150637e92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90075bd6cb2b41898fe9f295cffe091b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}